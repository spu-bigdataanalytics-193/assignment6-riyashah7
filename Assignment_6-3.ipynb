{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cS9tEuBejIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting up Spark environment\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSKJdE-ahPiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kvsRYRoiFam",
        "colab_type": "code",
        "outputId": "ffb6c638-1448-49b9-995d-6baab73db502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mounting on Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt5j0Fj3igmK",
        "colab_type": "code",
        "outputId": "c44cbb70-d038-4b09-bcdf-0ec83a312fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.5)\n",
            "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kidXCfMiitRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUAkV6T3hKmO",
        "colab_type": "text"
      },
      "source": [
        "**1. Explore configuration options on SparkSession**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3YVwjSzi7ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building Spark session with config details\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"Assignment_6\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .config(\"driver.cores\", \"1\") \\\n",
        "    .config(\"broadcast.compress\", \"true\") \\\n",
        "    .config(\"checkpoint.compress\", \"false\") \\\n",
        "    .config(\"streaming.backpressure.enabled\", \"false\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igbz7kt72dI3",
        "colab_type": "text"
      },
      "source": [
        " **2. READING DATA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YCz2Lf108my",
        "colab_type": "text"
      },
      "source": [
        "READ DATA AS DATAFRAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyLpLB0lkbNL",
        "colab_type": "text"
      },
      "source": [
        "Read a csv file into spark dataframe, considering first row as headers. A csv file is a comma seperated file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGUa4VbLpRl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read data into dataframe\n",
        "\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/content/sample_data/california_housing_test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxx9SBGemtOL",
        "colab_type": "code",
        "outputId": "0d6622d9-121e-4b0c-89e7-1ecfbd759062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Look at the dataframe\n",
        "\n",
        "df.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population| households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000| 606.000000|     6.608500|     344700.000000|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000| 277.000000|     3.599000|     176500.000000|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000| 495.000000|     5.793400|     270500.000000|\n",
            "|-118.360000|33.820000|         28.000000|  67.000000|     15.000000|  49.000000|  11.000000|     6.135900|     330000.000000|\n",
            "|-119.670000|36.330000|         19.000000|1241.000000|    244.000000| 850.000000| 237.000000|     2.937500|      81700.000000|\n",
            "|-119.560000|36.510000|         37.000000|1018.000000|    213.000000| 663.000000| 204.000000|     1.663500|      67000.000000|\n",
            "|-121.430000|38.630000|         43.000000|1009.000000|    225.000000| 604.000000| 218.000000|     1.664100|      67000.000000|\n",
            "|-120.650000|35.480000|         19.000000|2310.000000|    471.000000|1341.000000| 441.000000|     3.225000|     166900.000000|\n",
            "|-122.840000|38.400000|         15.000000|3080.000000|    617.000000|1446.000000| 599.000000|     3.669600|     194400.000000|\n",
            "|-118.020000|34.080000|         31.000000|2402.000000|    632.000000|2830.000000| 603.000000|     2.333300|     164200.000000|\n",
            "|-118.240000|33.980000|         45.000000| 972.000000|    249.000000|1288.000000| 261.000000|     2.205400|     125000.000000|\n",
            "|-119.120000|35.850000|         37.000000| 736.000000|    166.000000| 564.000000| 138.000000|     2.416700|      58300.000000|\n",
            "|-121.930000|37.250000|         36.000000|1089.000000|    182.000000| 535.000000| 170.000000|     4.690000|     252600.000000|\n",
            "|-117.030000|32.970000|         16.000000|3936.000000|    694.000000|1935.000000| 659.000000|     4.562500|     231200.000000|\n",
            "|-117.970000|33.730000|         27.000000|2097.000000|    325.000000|1217.000000| 331.000000|     5.712100|     222500.000000|\n",
            "|-117.990000|33.810000|         42.000000| 161.000000|     40.000000| 157.000000|  50.000000|     2.200000|     153100.000000|\n",
            "|-120.810000|37.530000|         15.000000| 570.000000|    123.000000| 189.000000| 107.000000|     1.875000|     181300.000000|\n",
            "|-121.200000|38.690000|         26.000000|3077.000000|    607.000000|1603.000000| 595.000000|     2.717400|     137500.000000|\n",
            "|-118.880000|34.210000|         26.000000|1590.000000|    196.000000| 654.000000| 199.000000|     6.585100|     300000.000000|\n",
            "|-122.590000|38.010000|         35.000000|8814.000000|   1307.000000|3450.000000|1258.000000|     6.172400|     414300.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+-----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvPN3LkKqL2I",
        "colab_type": "text"
      },
      "source": [
        "READING DATA AS RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-KlSoEpdqqQ",
        "colab_type": "text"
      },
      "source": [
        "Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. It is an immutable distributed collection of objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5rsZAnKqOWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read csv to RDD\n",
        "\n",
        "rdd_1 = spark.sparkContext.textFile(\"/content/sample_data/california_housing_test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIwojaRkq4Xl",
        "colab_type": "code",
        "outputId": "adacf5e1-0f21-4a1d-da5c-b8591162164f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# View RDD top 5 records\n",
        "\n",
        "rdd_1.take(5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"',\n",
              " '-122.050000,37.370000,27.000000,3885.000000,661.000000,1537.000000,606.000000,6.608500,344700.000000',\n",
              " '-118.300000,34.260000,43.000000,1510.000000,310.000000,809.000000,277.000000,3.599000,176500.000000',\n",
              " '-117.810000,33.780000,27.000000,3589.000000,507.000000,1484.000000,495.000000,5.793400,270500.000000',\n",
              " '-118.360000,33.820000,28.000000,67.000000,15.000000,49.000000,11.000000,6.135900,330000.000000']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5GcKEsPxWuk",
        "colab_type": "text"
      },
      "source": [
        "CONVERT RDD TO SPARK DATAFRAME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CTeyTsCx8h3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKVz1q9QxYfL",
        "colab_type": "code",
        "outputId": "c6c97009-6996-47a0-a32a-8aef12b3f530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "#Converting to dataframe\n",
        "\n",
        "get_data = rdd_1.map(lambda x : x.split(\",\"))\n",
        "dataframe = get_data.map(lambda p: Row(longitude=p[0],latitude=p[1],housing_median_age=p[2],total_rooms=p[3],total_bedrooms=p[4],population=p[5],households=p[6],median_income=p[7],median_house_value=p[8]))\n",
        "rdd_df = spark.createDataFrame(dataframe)\n",
        "rdd_df.show(10)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+--------------------+----------+-----------+--------------------+---------------+------------+----------------+-------------+\n",
            "|  households|  housing_median_age|  latitude|  longitude|  median_house_value|  median_income|  population|  total_bedrooms|  total_rooms|\n",
            "+------------+--------------------+----------+-----------+--------------------+---------------+------------+----------------+-------------+\n",
            "|\"households\"|\"housing_median_age\"|\"latitude\"|\"longitude\"|\"median_house_value\"|\"median_income\"|\"population\"|\"total_bedrooms\"|\"total_rooms\"|\n",
            "|  606.000000|           27.000000| 37.370000|-122.050000|       344700.000000|       6.608500| 1537.000000|      661.000000|  3885.000000|\n",
            "|  277.000000|           43.000000| 34.260000|-118.300000|       176500.000000|       3.599000|  809.000000|      310.000000|  1510.000000|\n",
            "|  495.000000|           27.000000| 33.780000|-117.810000|       270500.000000|       5.793400| 1484.000000|      507.000000|  3589.000000|\n",
            "|   11.000000|           28.000000| 33.820000|-118.360000|       330000.000000|       6.135900|   49.000000|       15.000000|    67.000000|\n",
            "|  237.000000|           19.000000| 36.330000|-119.670000|        81700.000000|       2.937500|  850.000000|      244.000000|  1241.000000|\n",
            "|  204.000000|           37.000000| 36.510000|-119.560000|        67000.000000|       1.663500|  663.000000|      213.000000|  1018.000000|\n",
            "|  218.000000|           43.000000| 38.630000|-121.430000|        67000.000000|       1.664100|  604.000000|      225.000000|  1009.000000|\n",
            "|  441.000000|           19.000000| 35.480000|-120.650000|       166900.000000|       3.225000| 1341.000000|      471.000000|  2310.000000|\n",
            "|  599.000000|           15.000000| 38.400000|-122.840000|       194400.000000|       3.669600| 1446.000000|      617.000000|  3080.000000|\n",
            "+------------+--------------------+----------+-----------+--------------------+---------------+------------+----------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKsEW3Hb0i1u",
        "colab_type": "text"
      },
      "source": [
        "CONVERT SPARK DATAFRAME TO RDD "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O-whp0b0lc_",
        "colab_type": "code",
        "outputId": "126c22f1-48ec-47ba-bac6-8081aa9fa7e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating dataframe from the previously generated RDD\n",
        "\n",
        "df_rdd1 = df.rdd\n",
        "df_rdd1"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MapPartitionsRDD[38] at javaToPython at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqRSbj6M1q3c",
        "colab_type": "text"
      },
      "source": [
        "CONVERT SPARK DATAFRAME TO PANDAS DATAFRAME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1EtwozlziAU",
        "colab_type": "code",
        "outputId": "64505934-0e59-449a-f571-321bf53598c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# Converting the spark dataframe into pandas dataframe\n",
        "\n",
        "pandas_df = df.toPandas()\n",
        "pandas_df"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.050000</td>\n",
              "      <td>37.370000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>3885.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>1537.000000</td>\n",
              "      <td>606.000000</td>\n",
              "      <td>6.608500</td>\n",
              "      <td>344700.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-118.300000</td>\n",
              "      <td>34.260000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>1510.000000</td>\n",
              "      <td>310.000000</td>\n",
              "      <td>809.000000</td>\n",
              "      <td>277.000000</td>\n",
              "      <td>3.599000</td>\n",
              "      <td>176500.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-117.810000</td>\n",
              "      <td>33.780000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>3589.000000</td>\n",
              "      <td>507.000000</td>\n",
              "      <td>1484.000000</td>\n",
              "      <td>495.000000</td>\n",
              "      <td>5.793400</td>\n",
              "      <td>270500.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-118.360000</td>\n",
              "      <td>33.820000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>6.135900</td>\n",
              "      <td>330000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-119.670000</td>\n",
              "      <td>36.330000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>1241.000000</td>\n",
              "      <td>244.000000</td>\n",
              "      <td>850.000000</td>\n",
              "      <td>237.000000</td>\n",
              "      <td>2.937500</td>\n",
              "      <td>81700.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>-119.860000</td>\n",
              "      <td>34.420000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1450.000000</td>\n",
              "      <td>642.000000</td>\n",
              "      <td>1258.000000</td>\n",
              "      <td>607.000000</td>\n",
              "      <td>1.179000</td>\n",
              "      <td>225000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>-118.140000</td>\n",
              "      <td>34.060000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>5257.000000</td>\n",
              "      <td>1082.000000</td>\n",
              "      <td>3496.000000</td>\n",
              "      <td>1036.000000</td>\n",
              "      <td>3.390600</td>\n",
              "      <td>237200.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>-119.700000</td>\n",
              "      <td>36.300000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>956.000000</td>\n",
              "      <td>201.000000</td>\n",
              "      <td>693.000000</td>\n",
              "      <td>220.000000</td>\n",
              "      <td>2.289500</td>\n",
              "      <td>62000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>-117.120000</td>\n",
              "      <td>34.100000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>3.270800</td>\n",
              "      <td>162500.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>-119.630000</td>\n",
              "      <td>34.420000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>1765.000000</td>\n",
              "      <td>263.000000</td>\n",
              "      <td>753.000000</td>\n",
              "      <td>260.000000</td>\n",
              "      <td>8.560800</td>\n",
              "      <td>500001.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3000 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        longitude   latitude  ... median_income median_house_value\n",
              "0     -122.050000  37.370000  ...      6.608500      344700.000000\n",
              "1     -118.300000  34.260000  ...      3.599000      176500.000000\n",
              "2     -117.810000  33.780000  ...      5.793400      270500.000000\n",
              "3     -118.360000  33.820000  ...      6.135900      330000.000000\n",
              "4     -119.670000  36.330000  ...      2.937500       81700.000000\n",
              "...           ...        ...  ...           ...                ...\n",
              "2995  -119.860000  34.420000  ...      1.179000      225000.000000\n",
              "2996  -118.140000  34.060000  ...      3.390600      237200.000000\n",
              "2997  -119.700000  36.300000  ...      2.289500       62000.000000\n",
              "2998  -117.120000  34.100000  ...      3.270800      162500.000000\n",
              "2999  -119.630000  34.420000  ...      8.560800      500001.000000\n",
              "\n",
              "[3000 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCmoTOJQ2n9m",
        "colab_type": "text"
      },
      "source": [
        " **3. TASKS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz7A9PJNl26i",
        "colab_type": "text"
      },
      "source": [
        "Here, I performed 8 taskes on my dataset using pyspark.sql.dataframe functions and sql queries. In order to implement SQL queries, we either need to connect to our SQL server or use a temporary view. Here, I created a temporary view for SQL purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6Gl-j_M3HCR",
        "colab_type": "text"
      },
      "source": [
        "1. Select first 10 rows of dataset - pyspark.sql.dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqTjQqH2Omx",
        "colab_type": "code",
        "outputId": "a63e6025-d2ad-494a-9474-ebc210cdc5c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Looking at the dataframe's first 10 rows\n",
        "\n",
        "df.show(10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population|households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000|606.000000|     6.608500|     344700.000000|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000|277.000000|     3.599000|     176500.000000|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000|495.000000|     5.793400|     270500.000000|\n",
            "|-118.360000|33.820000|         28.000000|  67.000000|     15.000000|  49.000000| 11.000000|     6.135900|     330000.000000|\n",
            "|-119.670000|36.330000|         19.000000|1241.000000|    244.000000| 850.000000|237.000000|     2.937500|      81700.000000|\n",
            "|-119.560000|36.510000|         37.000000|1018.000000|    213.000000| 663.000000|204.000000|     1.663500|      67000.000000|\n",
            "|-121.430000|38.630000|         43.000000|1009.000000|    225.000000| 604.000000|218.000000|     1.664100|      67000.000000|\n",
            "|-120.650000|35.480000|         19.000000|2310.000000|    471.000000|1341.000000|441.000000|     3.225000|     166900.000000|\n",
            "|-122.840000|38.400000|         15.000000|3080.000000|    617.000000|1446.000000|599.000000|     3.669600|     194400.000000|\n",
            "|-118.020000|34.080000|         31.000000|2402.000000|    632.000000|2830.000000|603.000000|     2.333300|     164200.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22cIFoeXC93U",
        "colab_type": "text"
      },
      "source": [
        "1. Select first 10 rows of dataset - SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dZHF5KqCU0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a temperary view to implement SQL queries in Spark\n",
        "\n",
        "df.createOrReplaceTempView(\"df_sql\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmDYHh8r_WMI",
        "colab_type": "code",
        "outputId": "96a3b6fc-8411-4c27-cbee-be133b2ad238",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# First 10 rows using SQL\n",
        "\n",
        "task_1 = 'select * from df_sql LIMIT 10'\n",
        "spark.sql(task_1).show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population|households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000|606.000000|     6.608500|     344700.000000|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000|277.000000|     3.599000|     176500.000000|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000|495.000000|     5.793400|     270500.000000|\n",
            "|-118.360000|33.820000|         28.000000|  67.000000|     15.000000|  49.000000| 11.000000|     6.135900|     330000.000000|\n",
            "|-119.670000|36.330000|         19.000000|1241.000000|    244.000000| 850.000000|237.000000|     2.937500|      81700.000000|\n",
            "|-119.560000|36.510000|         37.000000|1018.000000|    213.000000| 663.000000|204.000000|     1.663500|      67000.000000|\n",
            "|-121.430000|38.630000|         43.000000|1009.000000|    225.000000| 604.000000|218.000000|     1.664100|      67000.000000|\n",
            "|-120.650000|35.480000|         19.000000|2310.000000|    471.000000|1341.000000|441.000000|     3.225000|     166900.000000|\n",
            "|-122.840000|38.400000|         15.000000|3080.000000|    617.000000|1446.000000|599.000000|     3.669600|     194400.000000|\n",
            "|-118.020000|34.080000|         31.000000|2402.000000|    632.000000|2830.000000|603.000000|     2.333300|     164200.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9dpSIs23U-_",
        "colab_type": "text"
      },
      "source": [
        "2. Show the schema of of the dataset - pyspark.sql.dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN7ZJ4mmmnTS",
        "colab_type": "text"
      },
      "source": [
        "Looking at the datatypes and nullable constraints for each column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqx60kgL3Kyy",
        "colab_type": "code",
        "outputId": "c2b7ef98-b04e-435d-88d9-22330f227b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Schema of our data using pyspark\n",
        "\n",
        "df.printSchema()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- longitude: string (nullable = true)\n",
            " |-- latitude: string (nullable = true)\n",
            " |-- housing_median_age: string (nullable = true)\n",
            " |-- total_rooms: string (nullable = true)\n",
            " |-- total_bedrooms: string (nullable = true)\n",
            " |-- population: string (nullable = true)\n",
            " |-- households: string (nullable = true)\n",
            " |-- median_income: string (nullable = true)\n",
            " |-- median_house_value: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6u8TZFIDf_J",
        "colab_type": "text"
      },
      "source": [
        "2. Show the schema of of the dataset - SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrgq7JIdDjZE",
        "colab_type": "code",
        "outputId": "4011fbed-62e9-4110-a5f9-a3b69671b16d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Schema using Sql's describe method using SQL\n",
        " \n",
        "task_2 = \"DESCRIBE FORMATTED df_sql\"\n",
        "spark.sql(task_2).show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+---------+-------+\n",
            "|          col_name|data_type|comment|\n",
            "+------------------+---------+-------+\n",
            "|         longitude|   string|   null|\n",
            "|          latitude|   string|   null|\n",
            "|housing_median_age|   string|   null|\n",
            "|       total_rooms|   string|   null|\n",
            "|    total_bedrooms|   string|   null|\n",
            "|        population|   string|   null|\n",
            "|        households|   string|   null|\n",
            "|     median_income|   string|   null|\n",
            "|median_house_value|   string|   null|\n",
            "+------------------+---------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzLkThDu5UGs",
        "colab_type": "text"
      },
      "source": [
        "3. Group by and get max, min, count of a column in the dataset - pyspark.sql.dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0-GeufPxJHa",
        "colab_type": "text"
      },
      "source": [
        "The GROUP BY statement groups rows that have the same values into summary rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aTwyDS73Zf0",
        "colab_type": "code",
        "outputId": "51516973-33d2-4aa6-acfb-4afd87630d69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "# count of each income class and it's min and max values using pyspark\n",
        "\n",
        "df.groupBy('median_income').count().show()\n",
        "df.select(max('median_income'),min('median_income')).show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-----+\n",
            "|median_income|count|\n",
            "+-------------+-----+\n",
            "|     4.638900|    1|\n",
            "|     4.693000|    1|\n",
            "|     5.898000|    1|\n",
            "|    10.598100|    1|\n",
            "|     0.740300|    1|\n",
            "|     3.272700|    1|\n",
            "|     4.931800|    1|\n",
            "|     2.306800|    1|\n",
            "|     2.401000|    1|\n",
            "|     4.163000|    1|\n",
            "|     3.209100|    1|\n",
            "|     1.442700|    1|\n",
            "|     2.021700|    1|\n",
            "|     4.736100|    2|\n",
            "|     4.786100|    1|\n",
            "|     2.853300|    1|\n",
            "|     5.273000|    1|\n",
            "|     1.000000|    1|\n",
            "|     4.033300|    1|\n",
            "|     3.956600|    1|\n",
            "+-------------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+------------------+------------------+\n",
            "|max(median_income)|min(median_income)|\n",
            "+------------------+------------------+\n",
            "|          9.870800|          0.499900|\n",
            "+------------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi5Zh_B9Ftqz",
        "colab_type": "text"
      },
      "source": [
        "3. Group by and get max, min, count of a column in the dataset - SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn7NSonVF00G",
        "colab_type": "code",
        "outputId": "0b3404fa-1d8e-4cf2-b384-20f620d81be5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "# Using groupby for median_income and max and min functions of SQL\n",
        "\n",
        "task_3 = \"select median_income, count(median_income) from df_sql group by median_income\"\n",
        "spark.sql(task_3).show()\n",
        "\n",
        "task_3_1 = \"select MAX(median_income) as max_median_income from df_sql\"\n",
        "spark.sql(task_3_1).show()\n",
        "\n",
        "task_3_2 = \"select MIN(median_income) as min_median_income from df_sql\"\n",
        "spark.sql(task_3_2).show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+--------------------+\n",
            "|median_income|count(median_income)|\n",
            "+-------------+--------------------+\n",
            "|     4.638900|                   1|\n",
            "|     4.693000|                   1|\n",
            "|     5.898000|                   1|\n",
            "|    10.598100|                   1|\n",
            "|     0.740300|                   1|\n",
            "|     3.272700|                   1|\n",
            "|     4.931800|                   1|\n",
            "|     2.306800|                   1|\n",
            "|     2.401000|                   1|\n",
            "|     4.163000|                   1|\n",
            "|     3.209100|                   1|\n",
            "|     1.442700|                   1|\n",
            "|     2.021700|                   1|\n",
            "|     4.736100|                   2|\n",
            "|     4.786100|                   1|\n",
            "|     2.853300|                   1|\n",
            "|     5.273000|                   1|\n",
            "|     1.000000|                   1|\n",
            "|     4.033300|                   1|\n",
            "|     3.956600|                   1|\n",
            "+-------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----------------+\n",
            "|max_median_income|\n",
            "+-----------------+\n",
            "|         9.870800|\n",
            "+-----------------+\n",
            "\n",
            "+-----------------+\n",
            "|min_median_income|\n",
            "+-----------------+\n",
            "|         0.499900|\n",
            "+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuLNqek45Zpe",
        "colab_type": "text"
      },
      "source": [
        "4. Filter your dataset by some conditions based on your column - pyspark.sql.dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe7oqiwgwrYL",
        "colab_type": "text"
      },
      "source": [
        "Analysis on a dataset based on condition is very helpful to extract specific information from the data. Let's have a look at one such conditional extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGQkqXo94mmu",
        "colab_type": "code",
        "outputId": "c3baf57e-ac05-4597-e53c-9123ceced592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# As seen above, the lowest is 0.49 and highest is 9.8. Let's look at incomes > 3 using pyspark\n",
        "\n",
        "df.filter(df['median_income'] > 3).show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+------------+--------------+-----------+-----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age| total_rooms|total_bedrooms| population| households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+------------+--------------+-----------+-----------+-------------+------------------+\n",
            "|-122.050000|37.370000|         27.000000| 3885.000000|    661.000000|1537.000000| 606.000000|     6.608500|     344700.000000|\n",
            "|-117.810000|33.780000|         27.000000| 3589.000000|    507.000000|1484.000000| 495.000000|     5.793400|     270500.000000|\n",
            "|-118.360000|33.820000|         28.000000|   67.000000|     15.000000|  49.000000|  11.000000|     6.135900|     330000.000000|\n",
            "|-121.930000|37.250000|         36.000000| 1089.000000|    182.000000| 535.000000| 170.000000|     4.690000|     252600.000000|\n",
            "|-117.030000|32.970000|         16.000000| 3936.000000|    694.000000|1935.000000| 659.000000|     4.562500|     231200.000000|\n",
            "|-117.970000|33.730000|         27.000000| 2097.000000|    325.000000|1217.000000| 331.000000|     5.712100|     222500.000000|\n",
            "|-118.880000|34.210000|         26.000000| 1590.000000|    196.000000| 654.000000| 199.000000|     6.585100|     300000.000000|\n",
            "|-122.590000|38.010000|         35.000000| 8814.000000|   1307.000000|3450.000000|1258.000000|     6.172400|     414300.000000|\n",
            "|-118.030000|34.160000|         36.000000| 1401.000000|    218.000000| 667.000000| 225.000000|     7.161500|     484700.000000|\n",
            "|-118.390000|33.990000|         32.000000| 2612.000000|    418.000000|1030.000000| 402.000000|     6.603000|     369200.000000|\n",
            "|-118.080000|34.550000|          5.000000|16181.000000|   2971.000000|8152.000000|2651.000000|     4.523700|     141800.000000|\n",
            "|-118.320000|33.940000|         38.000000| 1067.000000|    170.000000| 499.000000| 169.000000|     4.638900|     183800.000000|\n",
            "|-118.020000|33.920000|         34.000000| 1478.000000|    251.000000| 956.000000| 277.000000|     5.523800|     185300.000000|\n",
            "|-119.010000|34.230000|         11.000000| 5785.000000|   1035.000000|2760.000000| 985.000000|     4.693000|     232200.000000|\n",
            "|-116.920000|32.770000|         16.000000| 2770.000000|    406.000000|1269.000000| 429.000000|     6.678300|     275000.000000|\n",
            "|-118.060000|34.150000|         37.000000| 1980.000000|    226.000000| 697.000000| 226.000000|    15.000100|     500001.000000|\n",
            "|-118.230000|34.130000|         48.000000| 1308.000000|    286.000000| 835.000000| 294.000000|     4.289100|     214800.000000|\n",
            "|-117.240000|33.170000|          4.000000| 9998.000000|   1874.000000|3925.000000|1672.000000|     4.282600|     237500.000000|\n",
            "|-121.910000|37.440000|         24.000000| 1212.000000|    251.000000| 799.000000| 242.000000|     5.080800|     212500.000000|\n",
            "|-117.900000|34.090000|         39.000000| 1726.000000|    333.000000| 892.000000| 335.000000|     4.340900|     191800.000000|\n",
            "+-----------+---------+------------------+------------+--------------+-----------+-----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osjAfXZqG6Yg",
        "colab_type": "text"
      },
      "source": [
        "4. Filter your dataset by some conditions based on your column - SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhd6yen3HBHA",
        "colab_type": "code",
        "outputId": "03461fbb-2ee3-449a-fadd-2754090ac8d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Looking at the data with income > 3.0 using SQL\n",
        "\n",
        "task_4 = \"select * from df_sql where median_income > 3\"\n",
        "spark.sql(task_4).show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+------------+--------------+-----------+-----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age| total_rooms|total_bedrooms| population| households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+------------+--------------+-----------+-----------+-------------+------------------+\n",
            "|-122.050000|37.370000|         27.000000| 3885.000000|    661.000000|1537.000000| 606.000000|     6.608500|     344700.000000|\n",
            "|-117.810000|33.780000|         27.000000| 3589.000000|    507.000000|1484.000000| 495.000000|     5.793400|     270500.000000|\n",
            "|-118.360000|33.820000|         28.000000|   67.000000|     15.000000|  49.000000|  11.000000|     6.135900|     330000.000000|\n",
            "|-121.930000|37.250000|         36.000000| 1089.000000|    182.000000| 535.000000| 170.000000|     4.690000|     252600.000000|\n",
            "|-117.030000|32.970000|         16.000000| 3936.000000|    694.000000|1935.000000| 659.000000|     4.562500|     231200.000000|\n",
            "|-117.970000|33.730000|         27.000000| 2097.000000|    325.000000|1217.000000| 331.000000|     5.712100|     222500.000000|\n",
            "|-118.880000|34.210000|         26.000000| 1590.000000|    196.000000| 654.000000| 199.000000|     6.585100|     300000.000000|\n",
            "|-122.590000|38.010000|         35.000000| 8814.000000|   1307.000000|3450.000000|1258.000000|     6.172400|     414300.000000|\n",
            "|-118.030000|34.160000|         36.000000| 1401.000000|    218.000000| 667.000000| 225.000000|     7.161500|     484700.000000|\n",
            "|-118.390000|33.990000|         32.000000| 2612.000000|    418.000000|1030.000000| 402.000000|     6.603000|     369200.000000|\n",
            "|-118.080000|34.550000|          5.000000|16181.000000|   2971.000000|8152.000000|2651.000000|     4.523700|     141800.000000|\n",
            "|-118.320000|33.940000|         38.000000| 1067.000000|    170.000000| 499.000000| 169.000000|     4.638900|     183800.000000|\n",
            "|-118.020000|33.920000|         34.000000| 1478.000000|    251.000000| 956.000000| 277.000000|     5.523800|     185300.000000|\n",
            "|-119.010000|34.230000|         11.000000| 5785.000000|   1035.000000|2760.000000| 985.000000|     4.693000|     232200.000000|\n",
            "|-116.920000|32.770000|         16.000000| 2770.000000|    406.000000|1269.000000| 429.000000|     6.678300|     275000.000000|\n",
            "|-118.060000|34.150000|         37.000000| 1980.000000|    226.000000| 697.000000| 226.000000|    15.000100|     500001.000000|\n",
            "|-118.230000|34.130000|         48.000000| 1308.000000|    286.000000| 835.000000| 294.000000|     4.289100|     214800.000000|\n",
            "|-117.240000|33.170000|          4.000000| 9998.000000|   1874.000000|3925.000000|1672.000000|     4.282600|     237500.000000|\n",
            "|-121.910000|37.440000|         24.000000| 1212.000000|    251.000000| 799.000000| 242.000000|     5.080800|     212500.000000|\n",
            "|-117.900000|34.090000|         39.000000| 1726.000000|    333.000000| 892.000000| 335.000000|     4.340900|     191800.000000|\n",
            "+-----------+---------+------------------+------------+--------------+-----------+-----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLTyOX4M5glS",
        "colab_type": "text"
      },
      "source": [
        "5. Apply order by - pyspark.sql.dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPU-4SO5vGcG",
        "colab_type": "text"
      },
      "source": [
        "Ordering data based on a column values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPAm1ZKJ5z57",
        "colab_type": "code",
        "outputId": "22dbfe82-c05d-41b4-f5db-2246054c7728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Ordering the data based on the house values using pyspark\n",
        "\n",
        "df.select('total_rooms', 'total_bedrooms', 'median_house_value').orderBy('median_house_value').show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+--------------+------------------+\n",
            "|total_rooms|total_bedrooms|median_house_value|\n",
            "+-----------+--------------+------------------+\n",
            "|3854.000000|   1046.000000|     100000.000000|\n",
            "|2235.000000|    545.000000|     100000.000000|\n",
            "|1439.000000|    327.000000|     100000.000000|\n",
            "| 432.000000|     87.000000|     100000.000000|\n",
            "|1200.000000|    468.000000|     100000.000000|\n",
            "|2722.000000|    479.000000|     100000.000000|\n",
            "| 831.000000|    149.000000|     100000.000000|\n",
            "| 954.000000|    233.000000|     100000.000000|\n",
            "|1138.000000|    304.000000|     100000.000000|\n",
            "|2668.000000|    510.000000|     100000.000000|\n",
            "| 764.000000|    200.000000|     100000.000000|\n",
            "| 980.000000|    193.000000|     100000.000000|\n",
            "|1886.000000|    586.000000|     100000.000000|\n",
            "|3585.000000|    548.000000|     100100.000000|\n",
            "|4776.000000|   1082.000000|     100500.000000|\n",
            "|2581.000000|    499.000000|     100500.000000|\n",
            "|1724.000000|    432.000000|     100600.000000|\n",
            "|1477.000000|    264.000000|     100600.000000|\n",
            "|1521.000000|    352.000000|     100600.000000|\n",
            "| 754.000000|    200.000000|     100800.000000|\n",
            "+-----------+--------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on2QuFJGHX85",
        "colab_type": "text"
      },
      "source": [
        "5. Apply order by - SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOnSmDmCHa8q",
        "colab_type": "code",
        "outputId": "65bb2675-2bef-4b46-f4ee-1f40c5290aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Ordering the data by mdeian house values using SQL's order by\n",
        "\n",
        "task_5 = \"select total_rooms, total_bedrooms, median_house_value from df_sql order by median_house_value\"\n",
        "spark.sql(task_5).show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+--------------+------------------+\n",
            "|total_rooms|total_bedrooms|median_house_value|\n",
            "+-----------+--------------+------------------+\n",
            "|3854.000000|   1046.000000|     100000.000000|\n",
            "|2235.000000|    545.000000|     100000.000000|\n",
            "|1439.000000|    327.000000|     100000.000000|\n",
            "| 432.000000|     87.000000|     100000.000000|\n",
            "|1200.000000|    468.000000|     100000.000000|\n",
            "|2722.000000|    479.000000|     100000.000000|\n",
            "| 831.000000|    149.000000|     100000.000000|\n",
            "| 954.000000|    233.000000|     100000.000000|\n",
            "|1138.000000|    304.000000|     100000.000000|\n",
            "|2668.000000|    510.000000|     100000.000000|\n",
            "| 764.000000|    200.000000|     100000.000000|\n",
            "| 980.000000|    193.000000|     100000.000000|\n",
            "|1886.000000|    586.000000|     100000.000000|\n",
            "|3585.000000|    548.000000|     100100.000000|\n",
            "|4776.000000|   1082.000000|     100500.000000|\n",
            "|2581.000000|    499.000000|     100500.000000|\n",
            "|1724.000000|    432.000000|     100600.000000|\n",
            "|1477.000000|    264.000000|     100600.000000|\n",
            "|1521.000000|    352.000000|     100600.000000|\n",
            "| 754.000000|    200.000000|     100800.000000|\n",
            "+-----------+--------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgRqUlIf6Y6b",
        "colab_type": "text"
      },
      "source": [
        "6. Select distinct records by a column - pyspark.sql.dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNCrDKXKwSJJ",
        "colab_type": "text"
      },
      "source": [
        "Getting unique values for a column can be useful to find the non-repeating values. We will explore distinct values of the median incomes for our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YadebFYM6DTw",
        "colab_type": "code",
        "outputId": "7258abea-5674-4f05-e998-fc04462db136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# All distinct incomes using pyspark\n",
        "\n",
        "df.select('median_income').distinct().show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+\n",
            "|median_income|\n",
            "+-------------+\n",
            "|     4.638900|\n",
            "|     4.693000|\n",
            "|     5.898000|\n",
            "|    10.598100|\n",
            "|     0.740300|\n",
            "|     3.272700|\n",
            "|     4.931800|\n",
            "|     2.306800|\n",
            "|     2.401000|\n",
            "|     4.163000|\n",
            "|     3.209100|\n",
            "|     1.442700|\n",
            "|     2.021700|\n",
            "|     4.736100|\n",
            "|     4.786100|\n",
            "|     2.853300|\n",
            "|     5.273000|\n",
            "|     1.000000|\n",
            "|     4.033300|\n",
            "|     3.956600|\n",
            "+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOUILcCrIvRM",
        "colab_type": "text"
      },
      "source": [
        "6. Select distinct records by a column - SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV9WQ0xOI--C",
        "colab_type": "code",
        "outputId": "d0b0c632-445d-4d47-e92e-8a49370370e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# All distinct incomes using SQL\n",
        "\n",
        "task_6 = \"select distinct median_income from df_sql\"\n",
        "spark.sql(task_6).show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+\n",
            "|median_income|\n",
            "+-------------+\n",
            "|     4.638900|\n",
            "|     4.693000|\n",
            "|     5.898000|\n",
            "|    10.598100|\n",
            "|     0.740300|\n",
            "|     3.272700|\n",
            "|     4.931800|\n",
            "|     2.306800|\n",
            "|     2.401000|\n",
            "|     4.163000|\n",
            "|     3.209100|\n",
            "|     1.442700|\n",
            "|     2.021700|\n",
            "|     4.736100|\n",
            "|     4.786100|\n",
            "|     2.853300|\n",
            "|     5.273000|\n",
            "|     1.000000|\n",
            "|     4.033300|\n",
            "|     3.956600|\n",
            "+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB3F12pi72V5",
        "colab_type": "text"
      },
      "source": [
        "7. Transform the data type of column from string to float - pyspark.sql.dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjATc792s9z-",
        "colab_type": "text"
      },
      "source": [
        "In this data, all columns are of string datatype, but as seen in the description, we have float values in various columns as well, so we can cast the type of that column from string to integer or float."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dISfeGl7fr3",
        "colab_type": "code",
        "outputId": "5328f900-3bab-4960-cd1c-4d09a4640b62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Casting of column datatype from string to float using pyspark\n",
        "\n",
        "df_n = df.withColumn('median_income', col('median_income').cast((FloatType())))\n",
        "df_n.printSchema()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- longitude: string (nullable = true)\n",
            " |-- latitude: string (nullable = true)\n",
            " |-- housing_median_age: string (nullable = true)\n",
            " |-- total_rooms: string (nullable = true)\n",
            " |-- total_bedrooms: string (nullable = true)\n",
            " |-- population: string (nullable = true)\n",
            " |-- households: string (nullable = true)\n",
            " |-- median_income: float (nullable = true)\n",
            " |-- median_house_value: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM0cvFi9Jd5A",
        "colab_type": "text"
      },
      "source": [
        "7. Transform the data type of column from string to float - SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAcxU_Z9JkYv",
        "colab_type": "code",
        "outputId": "85b1c127-380c-4f95-babc-7f87f167df20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Type casting in SQL\n",
        "\n",
        "task_7 = \"select CAST(median_income as FLOAT) from df_sql\"\n",
        "task_7_column = spark.sql(task_7)\n",
        "\n",
        "task_7_column.dtypes"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('median_income', 'float')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDcZfLPZ-GF3",
        "colab_type": "text"
      },
      "source": [
        "8. Apply group by with having clause - pyspark.sql.dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HEFREgTv3se",
        "colab_type": "text"
      },
      "source": [
        "Grouping a column value (categorical) and sum of population based on those values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qMs-vTB9ChM",
        "colab_type": "code",
        "outputId": "2c049af1-74cb-4bb2-ae3c-887a02161bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Groupby with having using pyspark\n",
        "\n",
        "df.groupBy('median_income').agg(sum('population')).alias('pop_income').show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+---------------+\n",
            "|median_income|sum(population)|\n",
            "+-------------+---------------+\n",
            "|     4.638900|          499.0|\n",
            "|     4.693000|         2760.0|\n",
            "|     5.898000|         1055.0|\n",
            "|    10.598100|         1605.0|\n",
            "|     0.740300|         1046.0|\n",
            "|     3.272700|          709.0|\n",
            "|     4.931800|         1196.0|\n",
            "|     2.306800|          664.0|\n",
            "|     2.401000|         1121.0|\n",
            "|     4.163000|          660.0|\n",
            "|     3.209100|         2332.0|\n",
            "|     1.442700|         1623.0|\n",
            "|     2.021700|          956.0|\n",
            "|     4.736100|         3320.0|\n",
            "|     4.786100|         1309.0|\n",
            "|     2.853300|          439.0|\n",
            "|     5.273000|          829.0|\n",
            "|     1.000000|          133.0|\n",
            "|     4.033300|         2003.0|\n",
            "|     3.956600|         2272.0|\n",
            "+-------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSDtY1GeRp8q",
        "colab_type": "text"
      },
      "source": [
        "8. Apply group by with having clause - SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kISf2Y6b_HJE",
        "colab_type": "code",
        "outputId": "29352db1-c930-4549-ee43-76afeb50ed17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Groupby with having using SQL\n",
        "\n",
        "task_8 = 'SELECT median_income, sum(population) from df_sql GROUP BY median_income HAVING sum(population) > 660'\n",
        "spark.sql(task_8).show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-------------------------------+\n",
            "|median_income|sum(CAST(population AS DOUBLE))|\n",
            "+-------------+-------------------------------+\n",
            "|     4.693000|                         2760.0|\n",
            "|     5.898000|                         1055.0|\n",
            "|    10.598100|                         1605.0|\n",
            "|     0.740300|                         1046.0|\n",
            "|     3.272700|                          709.0|\n",
            "|     4.931800|                         1196.0|\n",
            "|     2.306800|                          664.0|\n",
            "|     2.401000|                         1121.0|\n",
            "|     3.209100|                         2332.0|\n",
            "|     1.442700|                         1623.0|\n",
            "|     2.021700|                          956.0|\n",
            "|     4.736100|                         3320.0|\n",
            "|     4.786100|                         1309.0|\n",
            "|     5.273000|                          829.0|\n",
            "|     4.033300|                         2003.0|\n",
            "|     3.956600|                         2272.0|\n",
            "|     2.932400|                          850.0|\n",
            "|     5.288100|                         3064.0|\n",
            "|     4.130300|                         2049.0|\n",
            "|     4.756900|                          706.0|\n",
            "+-------------+-------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}